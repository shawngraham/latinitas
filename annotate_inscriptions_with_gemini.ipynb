{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEbh-Fr0c9LR"
      },
      "source": [
        "# Annotate Latin Inscriptions with Gemini API\n",
        "\n",
        "This notebook provides a complete workflow for:\n",
        "1. Downloading epigraphic data from EDH (Epigraphic Database Heidelberg)\n",
        "2. Annotating inscriptions using Gemini Flash 2.5 API\n",
        "3. Validating annotation quality\n",
        "4. Saving results and committing to git\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Google AI API Key**: Get one at https://aistudio.google.com/app/apikey\n",
        "2. **Installed packages**: Run the setup cell below\n",
        "\n",
        "## Cost Estimate\n",
        "\n",
        "- Gemini Flash 2.5: ~$0.20-0.50 USD per 1000 inscriptions\n",
        "- Processing time: ~20-40 minutes per 1000 inscriptions (with 1s delay)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "key = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "KsNfoItkdDb-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXwMfqcJc9LZ"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Br7djDI_c9Lb"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q google-generativeai pandas requests\n",
        "\n",
        "# Install latinepi if not already installed\n",
        "# import os\n",
        "# if not os.path.exists('latinepi'):\n",
        "#     print(\"Installing latinepi package...\")\n",
        "#     !pip install -e . -q\n",
        "# else:\n",
        "#     print(\"latinepi already available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qcMaH6dsc9Lc",
        "outputId": "82d193ae-ca61-4423-b662-28d3caf8eec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Google Generative AI imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    print(\"‚úÖ Google Generative AI imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå Please run the installation cell above\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/shawngraham/latinepi/refs/heads/main/gemini_annotation_prompt.md"
      ],
      "metadata": {
        "id": "ZFCyWHXOdaMv",
        "outputId": "7943cbd2-818d-44a5-b905-ab5f9bb2836d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-18 23:05:10--  https://raw.githubusercontent.com/shawngraham/latinepi/refs/heads/main/gemini_annotation_prompt.md\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16333 (16K) [text/plain]\n",
            "Saving to: ‚Äògemini_annotation_prompt.md‚Äô\n",
            "\n",
            "\r          gemini_an   0%[                    ]       0  --.-KB/s               \rgemini_annotation_p 100%[===================>]  15.95K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-11-18 23:05:11 (16.3 MB/s) - ‚Äògemini_annotation_prompt.md‚Äô saved [16333/16333]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODyLlU8rc9Ld"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Usg3eC_uc9Ld",
        "outputId": "93f88a59-3d88-4a62-d854-c19f22b04ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ API key loaded (first 10 chars): ...\n",
            "\n",
            "üìã Configuration loaded:\n",
            "   Model: gemini-flash-lite-latest\n",
            "   Output: assets/gemini_annotations.jsonl\n",
            "   Checkpoint every: 50 inscriptions\n",
            "   API delay: 1.0s\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# CONFIGURATION - Adjust these settings\n",
        "# ========================================\n",
        "\n",
        "# Google AI API Key\n",
        "GOOGLE_AI_API_KEY = key #os.environ.get(\"GOOGLE_AI_API_KEY\", \"\")\n",
        "\n",
        "if not GOOGLE_AI_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  No API key found in environment.\")\n",
        "    print(\"   Option 1: Set environment variable: export GOOGLE_AI_API_KEY='your-key'\")\n",
        "    print(\"   Option 2: Enter it directly in the next cell\")\n",
        "else:\n",
        "    print(f\"‚úÖ API key loaded (first 10 chars): {GOOGLE_AI_API_KEY[:10]}...\")\n",
        "\n",
        "# Gemini model to use\n",
        "#GEMINI_MODEL = \"gemini-2.0-flash-exp\"  # Fast and cost-effective\n",
        "GEMINI_MODEL = \"gemini-flash-lite-latest\"\n",
        "# Alternative: \"gemini-1.5-pro\" for higher quality (more expensive)\n",
        "\n",
        "# Output paths\n",
        "OUTPUT_DIR = Path(\"assets\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "ANNOTATIONS_FILE = OUTPUT_DIR / \"gemini_annotations.jsonl\"\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"gemini_annotations.jsonl.tmp\"\n",
        "\n",
        "# Processing parameters\n",
        "SAVE_CHECKPOINT_EVERY = 50  # Save progress every N inscriptions\n",
        "API_DELAY = 1.0  # Seconds between API calls (rate limiting)\n",
        "MAX_RETRIES = 3  # Retry failed API calls\n",
        "\n",
        "print(\"\\nüìã Configuration loaded:\")\n",
        "print(f\"   Model: {GEMINI_MODEL}\")\n",
        "print(f\"   Output: {ANNOTATIONS_FILE}\")\n",
        "print(f\"   Checkpoint every: {SAVE_CHECKPOINT_EVERY} inscriptions\")\n",
        "print(f\"   API delay: {API_DELAY}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV_f1cFHc9Le"
      },
      "outputs": [],
      "source": [
        "# If you need to set API key directly (not from environment):\n",
        "# GOOGLE_AI_API_KEY = \"paste-your-key-here\"\n",
        "# genai.configure(api_key=GOOGLE_AI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_inscriptions_from_csv(csv_path: Path,\n",
        "                                text_column: str = \"clean_text\",\n",
        "                                id_column: str = \"id\") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load inscriptions from a CSV file with id and clean_text columns.\n",
        "\n",
        "    Args:\n",
        "        csv_path: Path to CSV file\n",
        "        text_column: Name of column containing cleaned text (default: \"clean_text\")\n",
        "        id_column: Name of column containing inscription IDs (default: \"id\")\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with keys: id, text, transcription\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    inscriptions = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Get raw values\n",
        "        inscription_id = row.get(id_column, f\"row_{idx}\")\n",
        "        clean_text = row.get(text_column, \"\")\n",
        "\n",
        "        # Handle NaN values\n",
        "        if pd.isna(clean_text):\n",
        "            clean_text = \"\"\n",
        "\n",
        "        # Only include if we have text\n",
        "        if clean_text and len(str(clean_text).strip()) > 2:\n",
        "            inscriptions.append({\n",
        "                \"id\": str(inscription_id),\n",
        "                \"text\": \"\",  # No diplomatic text in your CSV\n",
        "                \"transcription\": str(clean_text)  # This is what gets annotated\n",
        "            })\n",
        "\n",
        "    return inscriptions\n",
        "\n",
        "\n",
        "# Usage:\n",
        "CSV_FILE = Path(\"cleaned_inscriptions.csv\")\n",
        "inscriptions = load_inscriptions_from_csv(\n",
        "    CSV_FILE,\n",
        "    text_column=\"clean_text\",\n",
        "    id_column=\"id\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(inscriptions)} inscriptions from CSV\")"
      ],
      "metadata": {
        "id": "g5nwVzf1eYrt",
        "outputId": "782f09fd-ba87-4a93-99b1-1d36ae3d2cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 2000 inscriptions from CSV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inscriptions"
      ],
      "metadata": {
        "id": "0j8IhGWSe9Eo",
        "outputId": "9d9b7443-cdff-4929-b466-14ce82dec1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIzzjKUuc9Lf"
      },
      "source": [
        "## 3. Download Inscription Data from EDH\n",
        "\n",
        "We'll use the `latinepi` package to search and download inscriptions from the Epigraphic Database Heidelberg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZNPx49Kc9Lg"
      },
      "outputs": [],
      "source": [
        "# EDH Search Parameters\n",
        "# Adjust these to get different inscription sets\n",
        "\n",
        "SEARCH_PARAMS = {\n",
        "    \"year_from\": 1,      # 1 CE\n",
        "    \"year_to\": 100,      # 100 CE (1st century)\n",
        "    \"limit\": 2000,       # Number of inscriptions to download\n",
        "}\n",
        "\n",
        "DOWNLOAD_DIR = Path(\"edh_downloads/batch_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
        "\n",
        "print(f\"üîç Search parameters:\")\n",
        "for key, value in SEARCH_PARAMS.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "print(f\"\\nüìÇ Will download to: {DOWNLOAD_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU8AQqyec9Li"
      },
      "outputs": [],
      "source": [
        "# Download inscriptions from EDH using latinepi command-line tool\n",
        "import subprocess\n",
        "\n",
        "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cmd = [\n",
        "    \"latinepi\",\n",
        "    \"--search-edh\",\n",
        "    \"--search-year-from\", str(SEARCH_PARAMS[\"year_from\"]),\n",
        "    \"--search-year-to\", str(SEARCH_PARAMS[\"year_to\"]),\n",
        "    \"--search-limit\", str(SEARCH_PARAMS[\"limit\"]),\n",
        "    \"--download-dir\", str(DOWNLOAD_DIR),\n",
        "]\n",
        "\n",
        "print(f\"‚è≥ Downloading {SEARCH_PARAMS['limit']} inscriptions from EDH...\")\n",
        "print(f\"   Command: {' '.join(cmd)}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
        "    print(result.stdout)\n",
        "    if result.stderr:\n",
        "        print(\"Errors/Warnings:\")\n",
        "        print(result.stderr)\n",
        "\n",
        "    # Count downloaded files\n",
        "    json_files = list(DOWNLOAD_DIR.glob(\"*.json\"))\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"‚úÖ Downloaded {len(json_files)} inscription files\")\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚ö†Ô∏è  Download timed out after 5 minutes\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during download: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVMhI20Vc9Lk"
      },
      "source": [
        "## 4. Load and Prepare Inscription Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "863xrC9Uc9Ll",
        "outputId": "c8a0350d-6576-4090-95d0-162f0d343b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Loaded 2000 inscriptions with valid text\n",
            "\n",
            "üìã Sample inscription:\n",
            "   ID: HD000052\n",
            "   Text: Pro salute et reditu et victorias! domini nostri Imperatoris Caesaris Marci Aure...\n",
            "   Length: 277 characters\n"
          ]
        }
      ],
      "source": [
        "def clean_leiden_text(text):\n",
        "    \"\"\"\n",
        "    Clean Leiden convention markup from inscription text.\n",
        "    Converts diplomatic transcription to plain Latin text.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove lost text markers\n",
        "    text = re.sub(r\"\\[\\s*-+\\??\\s*\\]\", \"\", text)\n",
        "    text = re.sub(r\"-+\\]\", \"\", text)\n",
        "    text = re.sub(r\"\\[-+\", \"\", text)\n",
        "\n",
        "    # Replace line breaks with spaces\n",
        "    text = text.replace(\"/\", \" \")\n",
        "\n",
        "    # Remove Leiden markup\n",
        "    text = text.replace(\"(\", \"\").replace(\")\", \"\")  # Abbreviations\n",
        "    text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # Restorations\n",
        "    text = text.replace(\"?\", \"\")  # Uncertain readings\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_inscriptions_from_json_dir(json_dir: Path) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load and clean inscriptions from a directory of JSON files.\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with keys: id, text, transcription\n",
        "    \"\"\"\n",
        "    json_files = list(json_dir.glob(\"*.json\"))\n",
        "\n",
        "    inscriptions = []\n",
        "\n",
        "    for json_file in json_files:\n",
        "        try:\n",
        "            with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Get diplomatic text and transcription\n",
        "            raw_text = data.get(\"diplomatic_text\", \"\")\n",
        "            transcription = data.get(\"transcription\", \"\")\n",
        "\n",
        "            # Clean the transcription\n",
        "            cleaned = clean_leiden_text(transcription)\n",
        "\n",
        "            # Only include if we have text\n",
        "            if cleaned and len(cleaned.strip()) > 2:\n",
        "                inscriptions.append({\n",
        "                    \"id\": data.get(\"id\", json_file.stem),\n",
        "                    \"text\": raw_text,\n",
        "                    \"transcription\": cleaned\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error loading {json_file.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return inscriptions\n",
        "\n",
        "\n",
        "# Load inscriptions\n",
        "#print(f\"üìö Loading inscriptions from {DOWNLOAD_DIR}...\")\n",
        "\n",
        "CSV_FILE = Path(\"cleaned_inscriptions.csv\")\n",
        "inscriptions = load_inscriptions_from_csv(\n",
        "    CSV_FILE,\n",
        "    text_column=\"clean_text\",\n",
        "    id_column=\"id\"\n",
        ")\n",
        "#inscriptions = load_inscriptions_from_json_dir(DOWNLOAD_DIR)\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(inscriptions)} inscriptions with valid text\")\n",
        "\n",
        "# Show sample\n",
        "if inscriptions:\n",
        "    print(\"\\nüìã Sample inscription:\")\n",
        "    sample = inscriptions[0]\n",
        "    print(f\"   ID: {sample['id']}\")\n",
        "    print(f\"   Text: {sample['transcription'][:80]}...\")\n",
        "    print(f\"   Length: {len(sample['transcription'])} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX5tyGPMc9Ll"
      },
      "source": [
        "## 5. Load Annotation Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BJUQ-P3Kc9Lm",
        "outputId": "d898b7db-fb94-468a-a08e-9bc39e867b9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded annotation prompt from gemini_annotation_prompt.md\n",
            "   Prompt length: 16238 characters\n",
            "   Estimated tokens: ~3062\n"
          ]
        }
      ],
      "source": [
        "# Load the annotation prompt\n",
        "PROMPT_FILE = Path(\"gemini_annotation_prompt.md\")\n",
        "\n",
        "if not PROMPT_FILE.exists():\n",
        "    print(f\"‚ùå Prompt file not found: {PROMPT_FILE}\")\n",
        "    print(\"   Make sure you're running this notebook from the repository root\")\n",
        "else:\n",
        "    with open(PROMPT_FILE, 'r', encoding='utf-8') as f:\n",
        "        ANNOTATION_PROMPT = f.read()\n",
        "\n",
        "    print(f\"‚úÖ Loaded annotation prompt from {PROMPT_FILE}\")\n",
        "    print(f\"   Prompt length: {len(ANNOTATION_PROMPT)} characters\")\n",
        "    print(f\"   Estimated tokens: ~{len(ANNOTATION_PROMPT.split()) * 1.3:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_XWy6rLc9Lm"
      },
      "source": [
        "## 6. Initialize Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hLcmSrVgc9Ln",
        "outputId": "af111968-5424-41e1-d27f-7b5da850e4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API configured\n",
            "   Model: gemini-flash-lite-latest\n",
            "   Temperature: 0.1 (deterministic)\n",
            "\n",
            "üß™ Testing API connection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-flash-lite-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2982.65ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Response: API connection successful\n",
            "   ‚úÖ API is working!\n"
          ]
        }
      ],
      "source": [
        "# Configure Gemini API\n",
        "if not GOOGLE_AI_API_KEY:\n",
        "    raise ValueError(\n",
        "        \"No API key set! Either:\\n\"\n",
        "        \"1. Set environment variable: export GOOGLE_AI_API_KEY='your-key'\\n\"\n",
        "        \"2. Or set it directly in the configuration cell above\"\n",
        "    )\n",
        "\n",
        "genai.configure(api_key=GOOGLE_AI_API_KEY)\n",
        "\n",
        "# Initialize the model\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=GEMINI_MODEL,\n",
        "    generation_config={\n",
        "        \"temperature\": 0.1,  # Low temperature for deterministic output\n",
        "        \"top_p\": 0.95,\n",
        "        \"top_k\": 40,\n",
        "        \"max_output_tokens\": 2048,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Gemini API configured\")\n",
        "print(f\"   Model: {GEMINI_MODEL}\")\n",
        "print(f\"   Temperature: 0.1 (deterministic)\")\n",
        "print(f\"\\nüß™ Testing API connection...\")\n",
        "\n",
        "try:\n",
        "    test_response = model.generate_content(\"Say 'API connection successful' in exactly those words.\")\n",
        "    print(f\"   Response: {test_response.text.strip()}\")\n",
        "    print(\"   ‚úÖ API is working!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå API test failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKxXoX1oc9Ln"
      },
      "source": [
        "## 7. Annotation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qGt90nITc9Lo",
        "outputId": "87c16476-a841-48bd-db64-83eff051fa59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Annotation functions defined\n"
          ]
        }
      ],
      "source": [
        "def annotate_single_inscription(inscription: Dict, prompt: str, model) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Annotate a single inscription using Gemini API.\n",
        "\n",
        "    Args:\n",
        "        inscription: Dict with keys: id, text, transcription\n",
        "        prompt: The annotation prompt (system instructions)\n",
        "        model: Gemini model instance\n",
        "\n",
        "    Returns:\n",
        "        Dict with annotations added, or None if failed\n",
        "    \"\"\"\n",
        "    # Prepare input JSON\n",
        "    input_json = json.dumps({\n",
        "        \"id\": inscription.get(\"id\", \"\"),\n",
        "        \"text\": inscription.get(\"text\", \"\"),\n",
        "        \"transcription\": inscription.get(\"transcription\", \"\")\n",
        "    }, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Construct full prompt\n",
        "    full_prompt = f\"\"\"{prompt}\n",
        "\n",
        "---\n",
        "\n",
        "Please annotate the following inscription:\n",
        "\n",
        "{input_json}\n",
        "\n",
        "Return ONLY the JSON object with annotations added. No other text.\n",
        "\"\"\"\n",
        "\n",
        "    # Try API call with retries\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            response = model.generate_content(full_prompt)\n",
        "            response_text = response.text.strip()\n",
        "\n",
        "            # Clean up response\n",
        "            if response_text.startswith(\"```json\"):\n",
        "                response_text = response_text[7:]\n",
        "            if response_text.startswith(\"```\"):\n",
        "                response_text = response_text[3:]\n",
        "            if response_text.endswith(\"```\"):\n",
        "                response_text = response_text[:-3]\n",
        "            response_text = response_text.strip()\n",
        "\n",
        "            # Parse JSON\n",
        "            result = json.loads(response_text)\n",
        "\n",
        "            # Validate structure\n",
        "            if \"annotations\" not in result:\n",
        "                print(f\"‚ö†Ô∏è  Missing 'annotations' field for {inscription.get('id')}\")\n",
        "                return None\n",
        "\n",
        "            if not isinstance(result[\"annotations\"], list):\n",
        "                print(f\"‚ö†Ô∏è  Invalid annotations format for {inscription.get('id')}\")\n",
        "                return None\n",
        "\n",
        "            return result\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                continue\n",
        "            print(f\"‚ùå JSON parse error for {inscription.get('id')} after {MAX_RETRIES} attempts\")\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < MAX_RETRIES - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "                continue\n",
        "            print(f\"‚ùå API error for {inscription.get('id')}: {e}\")\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def save_checkpoint(results: List[Dict], checkpoint_file: Path):\n",
        "    \"\"\"Save intermediate results to checkpoint file.\"\"\"\n",
        "    with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "        for result in results:\n",
        "            f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
        "\n",
        "\n",
        "def save_final(results: List[Dict], output_file: Path, checkpoint_file: Path):\n",
        "    \"\"\"Save final results and clean up checkpoint.\"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for result in results:\n",
        "            # Remove any error markers\n",
        "            clean_result = {k: v for k, v in result.items() if k != \"_error\"}\n",
        "            f.write(json.dumps(clean_result, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    # Clean up checkpoint\n",
        "    if checkpoint_file.exists():\n",
        "        checkpoint_file.unlink()\n",
        "\n",
        "print(\"‚úÖ Annotation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6evUZy_c9Lo"
      },
      "source": [
        "## 8. Run Batch Annotation\n",
        "\n",
        "‚ö†Ô∏è **Important**: This cell will make API calls and may take 30-60 minutes for 2000 inscriptions.\n",
        "\n",
        "Progress is saved every 50 inscriptions, so you can interrupt and resume if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "e7Y0zUmic9Lp",
        "outputId": "e3a0cf3e-4d9d-4b0b-86a2-b7d2c81b9c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Processing all 2000 inscriptions\n",
            "\n",
            "üöÄ Starting annotation...\n",
            "   Total: 2000\n",
            "   Already completed: 0\n",
            "   Remaining: 2000\n",
            "   Save checkpoint every: 50\n",
            "   API delay: 1.0s\n",
            "\n",
            "======================================================================\n",
            "[1/2000 (0.1%)] HD000052... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-flash-lite-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1391.02ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-flash-lite-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1111.63ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-flash-lite-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 9651.87ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-flash-lite-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1466.61ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-flash-lite-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1241.44ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mServiceUnavailable\u001b[0m: 503 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-flash-lite-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: The model is overloaded. Please try again later.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3216685380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Annotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotate_single_inscription\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minscription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mANNOTATION_PROMPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-125128419.py\u001b[0m in \u001b[0;36mannotate_single_inscription\u001b[0;34m(inscription, prompt, model)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mresponse_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             )\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# if exception not raised, sleep before next attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_sleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Optional: Limit number of inscriptions for testing\n",
        "TEST_LIMIT = None  # Set to e.g. 10 for testing, None for all\n",
        "\n",
        "if TEST_LIMIT:\n",
        "    inscriptions_to_process = inscriptions[:TEST_LIMIT]\n",
        "    print(f\"‚ö†Ô∏è  TEST MODE: Processing only {TEST_LIMIT} inscriptions\")\n",
        "else:\n",
        "    inscriptions_to_process = inscriptions\n",
        "    print(f\"üìù Processing all {len(inscriptions)} inscriptions\")\n",
        "\n",
        "# Check for existing checkpoint\n",
        "completed = []\n",
        "resume_from = 0\n",
        "\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    print(f\"\\nüìÇ Found checkpoint file: {CHECKPOINT_FILE}\")\n",
        "    with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:\n",
        "        completed = [json.loads(line) for line in f]\n",
        "    resume_from = len(completed)\n",
        "    print(f\"   Resuming from index {resume_from}\")\n",
        "\n",
        "# Statistics\n",
        "stats = {\n",
        "    'start_time': datetime.now(),\n",
        "    'total': len(inscriptions_to_process),\n",
        "    'successful': len(completed),\n",
        "    'failed': 0,\n",
        "    'total_entities': sum(len(r.get('annotations', [])) for r in completed)\n",
        "}\n",
        "\n",
        "print(f\"\\nüöÄ Starting annotation...\")\n",
        "print(f\"   Total: {stats['total']}\")\n",
        "print(f\"   Already completed: {len(completed)}\")\n",
        "print(f\"   Remaining: {stats['total'] - resume_from}\")\n",
        "print(f\"   Save checkpoint every: {SAVE_CHECKPOINT_EVERY}\")\n",
        "print(f\"   API delay: {API_DELAY}s\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Process inscriptions\n",
        "for i, inscription in enumerate(inscriptions_to_process[resume_from:], start=resume_from):\n",
        "    inscription_id = inscription.get('id', f'index_{i}')\n",
        "\n",
        "    # Progress indicator\n",
        "    progress_pct = (i + 1) / stats['total'] * 100\n",
        "    print(f\"[{i+1}/{stats['total']} ({progress_pct:.1f}%)] {inscription_id}... \", end=\"\", flush=True)\n",
        "\n",
        "    # Annotate\n",
        "    result = annotate_single_inscription(inscription, ANNOTATION_PROMPT, model)\n",
        "\n",
        "    if result:\n",
        "        completed.append(result)\n",
        "        num_entities = len(result.get('annotations', []))\n",
        "        stats['successful'] += 1\n",
        "        stats['total_entities'] += num_entities\n",
        "        print(f\"‚úì ({num_entities} entities)\")\n",
        "    else:\n",
        "        # Save failed case with error marker\n",
        "        completed.append({\n",
        "            **inscription,\n",
        "            \"annotations\": [],\n",
        "            \"_error\": True\n",
        "        })\n",
        "        stats['failed'] += 1\n",
        "        print(\"‚úó (failed)\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (i + 1) % SAVE_CHECKPOINT_EVERY == 0:\n",
        "        save_checkpoint(completed, CHECKPOINT_FILE)\n",
        "        elapsed = (datetime.now() - stats['start_time']).total_seconds()\n",
        "        rate = (i + 1) / elapsed\n",
        "        remaining = (stats['total'] - i - 1) / rate if rate > 0 else 0\n",
        "        print(f\"   üíæ Checkpoint saved ({len(completed)} completed, ~{remaining/60:.1f} min remaining)\")\n",
        "\n",
        "    # Rate limiting\n",
        "    if i < stats['total'] - 1:\n",
        "        time.sleep(API_DELAY)\n",
        "\n",
        "# Save final results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üíæ Saving final results...\")\n",
        "save_final(completed, ANNOTATIONS_FILE, CHECKPOINT_FILE)\n",
        "\n",
        "# Final statistics\n",
        "elapsed = (datetime.now() - stats['start_time']).total_seconds()\n",
        "avg_entities = stats['total_entities'] / max(1, stats['successful'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ANNOTATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total inscriptions: {stats['total']}\")\n",
        "print(f\"Successful: {stats['successful']}\")\n",
        "print(f\"Failed: {stats['failed']}\")\n",
        "print(f\"Success rate: {stats['successful']/stats['total']*100:.1f}%\")\n",
        "print(f\"\\nTotal entities annotated: {stats['total_entities']}\")\n",
        "print(f\"Avg entities per inscription: {avg_entities:.1f}\")\n",
        "print(f\"\\nTime elapsed: {elapsed/60:.1f} minutes\")\n",
        "print(f\"Rate: {stats['total']/elapsed*60:.1f} inscriptions/minute\")\n",
        "print(f\"\\nOutput saved to: {ANNOTATIONS_FILE}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ro-syVNc9Lq"
      },
      "source": [
        "## 9. Validate Annotations\n",
        "\n",
        "Run quality checks on the annotated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQwyp30Dc9Lq"
      },
      "outputs": [],
      "source": [
        "# Quick validation\n",
        "print(\"üîç Running validation checks...\\n\")\n",
        "\n",
        "# Load results\n",
        "with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:\n",
        "    results = [json.loads(line) for line in f]\n",
        "\n",
        "# Count entity types\n",
        "entity_counts = Counter()\n",
        "errors = []\n",
        "overlapping = []\n",
        "\n",
        "for record in results:\n",
        "    text = record.get('transcription', '')\n",
        "    annotations = record.get('annotations', [])\n",
        "\n",
        "    if not annotations:\n",
        "        continue\n",
        "\n",
        "    # Count entity types\n",
        "    for ann in annotations:\n",
        "        if len(ann) == 3:\n",
        "            start, end, label = ann\n",
        "            entity_counts[label] += 1\n",
        "\n",
        "            # Check bounds\n",
        "            if start < 0 or end > len(text) or start >= end:\n",
        "                errors.append({\n",
        "                    'id': record.get('id'),\n",
        "                    'error': f'Invalid bounds [{start}:{end}] for text length {len(text)}'\n",
        "                })\n",
        "\n",
        "    # Check for overlaps\n",
        "    sorted_anns = sorted(annotations, key=lambda x: x[0])\n",
        "    for i in range(len(sorted_anns) - 1):\n",
        "        if sorted_anns[i][1] > sorted_anns[i+1][0]:  # end[i] > start[i+1]\n",
        "            overlapping.append({\n",
        "                'id': record.get('id'),\n",
        "                'overlap': f'{sorted_anns[i]} overlaps with {sorted_anns[i+1]}'\n",
        "            })\n",
        "\n",
        "# Print results\n",
        "print(\"üìä Entity Type Distribution:\")\n",
        "print(f\"{'Entity Type':<30} {'Count':>8}\")\n",
        "print(\"-\" * 40)\n",
        "for label, count in entity_counts.most_common():\n",
        "    print(f\"{label:<30} {count:8d}\")\n",
        "\n",
        "print(f\"\\nüîç Quality Checks:\")\n",
        "print(f\"   Total records: {len(results)}\")\n",
        "print(f\"   Records with annotations: {sum(1 for r in results if r.get('annotations'))}\")\n",
        "print(f\"   Total entities: {sum(entity_counts.values())}\")\n",
        "\n",
        "if errors:\n",
        "    print(f\"\\n‚ùå Found {len(errors)} errors:\")\n",
        "    for err in errors[:5]:\n",
        "        print(f\"   {err['id']}: {err['error']}\")\n",
        "    if len(errors) > 5:\n",
        "        print(f\"   ... and {len(errors) - 5} more\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ No boundary errors found\")\n",
        "\n",
        "if overlapping:\n",
        "    print(f\"\\n‚ö†Ô∏è  Found {len(overlapping)} overlapping spans:\")\n",
        "    for ovr in overlapping[:5]:\n",
        "        print(f\"   {ovr['id']}: {ovr['overlap']}\")\n",
        "    if len(overlapping) > 5:\n",
        "        print(f\"   ... and {len(overlapping) - 5} more\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ No overlapping spans found\")\n",
        "\n",
        "# Overall quality\n",
        "error_rate = len(errors) / max(1, len(results))\n",
        "overlap_rate = len(overlapping) / max(1, len(results))\n",
        "\n",
        "if error_rate == 0 and overlap_rate == 0:\n",
        "    quality = \"üåü EXCELLENT\"\n",
        "elif error_rate < 0.01 and overlap_rate < 0.05:\n",
        "    quality = \"‚úÖ GOOD\"\n",
        "elif error_rate < 0.05 and overlap_rate < 0.10:\n",
        "    quality = \"‚ö†Ô∏è  FAIR\"\n",
        "else:\n",
        "    quality = \"‚ùå NEEDS REVIEW\"\n",
        "\n",
        "print(f\"\\nüéØ Overall Quality: {quality}\")\n",
        "print(f\"   Error rate: {error_rate:.2%}\")\n",
        "print(f\"   Overlap rate: {overlap_rate:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGVflBDFc9Lq"
      },
      "source": [
        "## 10. Show Sample Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsV-Hy_Kc9Lq"
      },
      "outputs": [],
      "source": [
        "# Display a few sample annotations\n",
        "import random\n",
        "\n",
        "samples = [r for r in results if r.get('annotations')]\n",
        "if samples:\n",
        "    print(\"üìã Sample Annotated Inscriptions:\\n\")\n",
        "\n",
        "    for i, record in enumerate(random.sample(samples, min(3, len(samples))), 1):\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Sample {i}\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"ID: {record.get('id', 'N/A')}\")\n",
        "        print(f\"Text: {record['transcription']}\")\n",
        "        print(f\"\\nEntities ({len(record['annotations'])}):\")\n",
        "\n",
        "        for start, end, label in record['annotations'][:15]:  # Max 15 entities\n",
        "            entity_text = record['transcription'][start:end]\n",
        "            print(f\"  [{start:3d}:{end:3d}] {label:25s} = '{entity_text}'\")\n",
        "\n",
        "        if len(record['annotations']) > 15:\n",
        "            print(f\"  ... and {len(record['annotations']) - 15} more\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No annotated samples to display\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Lucungc9Lr"
      },
      "source": [
        "## 11. Commit and Push Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KflFp_zqc9Lr"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "# Generate commit message\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "commit_message = f\"\"\"Add Gemini-annotated inscriptions ({timestamp})\n",
        "\n",
        "Annotated {stats['successful']} inscriptions using {GEMINI_MODEL}\n",
        "- Total entities: {stats['total_entities']}\n",
        "- Avg entities per inscription: {avg_entities:.1f}\n",
        "- Success rate: {stats['successful']/stats['total']*100:.1f}%\n",
        "- Search params: {SEARCH_PARAMS['year_from']}-{SEARCH_PARAMS['year_to']} CE\n",
        "\n",
        "Files:\n",
        "- {ANNOTATIONS_FILE}\n",
        "- {DOWNLOAD_DIR}/ (raw EDH data)\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Commit message:\")\n",
        "print(\"=\" * 70)\n",
        "print(commit_message)\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "# Ask for confirmation\n",
        "confirm = input(\"Commit and push these files? (yes/no): \").strip().lower()\n",
        "\n",
        "if confirm == 'yes':\n",
        "    try:\n",
        "        # Check git status\n",
        "        print(\"\\nüìä Git status:\")\n",
        "        result = subprocess.run(['git', 'status', '--short'], capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "\n",
        "        # Add files\n",
        "        print(\"\\nüì¶ Adding files to git...\")\n",
        "        subprocess.run(['git', 'add', str(ANNOTATIONS_FILE)], check=True)\n",
        "        subprocess.run(['git', 'add', str(DOWNLOAD_DIR)], check=True)\n",
        "        print(\"   ‚úÖ Files staged\")\n",
        "\n",
        "        # Commit\n",
        "        print(\"\\nüíæ Creating commit...\")\n",
        "        subprocess.run(['git', 'commit', '-m', commit_message], check=True)\n",
        "        print(\"   ‚úÖ Commit created\")\n",
        "\n",
        "        # Get current branch\n",
        "        branch_result = subprocess.run(\n",
        "            ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        current_branch = branch_result.stdout.strip()\n",
        "\n",
        "        # Push\n",
        "        print(f\"\\nüöÄ Pushing to origin/{current_branch}...\")\n",
        "        subprocess.run(['git', 'push', 'origin', current_branch], check=True)\n",
        "        print(\"   ‚úÖ Pushed successfully\")\n",
        "\n",
        "        print(\"\\n‚úÖ All done!\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\\n‚ùå Git operation failed: {e}\")\n",
        "        print(\"   You may need to commit and push manually\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è  Skipping git operations\")\n",
        "    print(f\"   To commit later, run:\")\n",
        "    print(f\"   git add {ANNOTATIONS_FILE} {DOWNLOAD_DIR}\")\n",
        "    print(f\"   git commit -m 'Add Gemini annotations'\")\n",
        "    print(f\"   git push\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP6baqROc9Lr"
      },
      "source": [
        "## 12. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPf9bbhLc9Lr"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üéâ WORKFLOW COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"   Inscriptions downloaded: {len(json_files)}\")\n",
        "print(f\"   Inscriptions annotated: {stats['successful']}\")\n",
        "print(f\"   Total entities: {stats['total_entities']}\")\n",
        "print(f\"   Output file: {ANNOTATIONS_FILE}\")\n",
        "\n",
        "print(f\"\\nüî¨ Next Steps:\")\n",
        "print(f\"\\n1. Review annotation quality:\")\n",
        "print(f\"   python validate_annotations.py {ANNOTATIONS_FILE}\")\n",
        "\n",
        "print(f\"\\n2. Use in your training notebook:\")\n",
        "print(f\"   INPUT_FILE = '{ANNOTATIONS_FILE}'\")\n",
        "print(f\"   partition_data(INPUT_FILE, CLEAN_OUTPUT_FILE, FIX_OUTPUT_FILE)\")\n",
        "\n",
        "print(f\"\\n3. Compare with synthetic data:\")\n",
        "print(f\"   - Train model A: synthetic data only (463 examples)\")\n",
        "print(f\"   - Train model B: real data only ({stats['successful']} examples)\")\n",
        "print(f\"   - Train model C: combined\")\n",
        "print(f\"   - Evaluate all three on held-out real inscriptions\")\n",
        "\n",
        "print(f\"\\n4. If quality is good, annotate more:\")\n",
        "print(f\"   - Adjust SEARCH_PARAMS in cell 3\")\n",
        "print(f\"   - Download different time periods/regions\")\n",
        "print(f\"   - Aim for 5000+ total inscriptions\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

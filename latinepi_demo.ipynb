{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# Latin Epigraphic Inscription Parser (latinepi) - Complete Workflow Demo\n\nThis notebook demonstrates the complete workflow for extracting structured personal data from Roman Latin epigraphic inscriptions using the `latinepi` tool with Latin BERT.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shawngraham/latinitas/blob/main/latinepi_demo.ipynb)\n\n**Note on Google Colab Compatibility:** Google Colab currently uses Python 3.12, which is incompatible with Latin BERT (requires Python 3.8-3.10). The demo will work in stub mode without Latin BERT, or you can run it locally with Python 3.10 for full functionality.\n\n## Features Demonstrated\n\n1. **Installation** - Set up latinepi in Google Colab\n2. **Latin BERT Setup** - Install and configure the Latin BERT model (requires Python 3.10)\n3. **Basic Processing** - Extract entities from CSV and JSON files\n4. **EDH Integration** - Download inscriptions from Epigraphic Database Heidelberg\n5. **Bulk Search** - Search and download multiple inscriptions by criteria\n6. **Confidence Filtering** - Apply thresholds and flag ambiguous entities\n7. **Complete Pipeline** - Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n8. **Visualization** - Analyze and visualize the extracted data\n\n## About the Tool\n\n**latinepi** extracts prosopographical data from Latin inscriptions using Named Entity Recognition:\n- Personal names (praenomen, nomen, cognomen)\n- Social status markers\n- Locations\n- Ages, occupations, military service\n- And more!\n\nThe tool can use **Latin BERT** (https://github.com/dbamman/latin-bert/), a transformer model trained on 642.7M tokens of Latin text for accurate entity extraction. However, it works in stub mode without Latin BERT for demonstration purposes.\n\nRepository: https://github.com/shawngraham/latinitas"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, let's clone the repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/shawngraham/latinitas.git\n",
    "%cd latinitas\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q pandas requests\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 2a. Optional: Install Latin BERT for Enhanced Accuracy\n\nThe tool can use a pre-trained Latin BERT model for more accurate named entity recognition. This installs the Latin BERT model from https://github.com/dbamman/latin-bert/\n\n**CRITICAL: Python Version Requirement**\n- Latin BERT requires Python 3.8, 3.9, or 3.10 (NOT 3.11 or 3.12)\n- **Google Colab now uses Python 3.12** - you will encounter errors\n- The notebook will check your version and provide workarounds\n\n**Workaround Options:**\n1. **Skip Latin BERT** - The tool works without it (uses stub mode for demo purposes)\n2. **Local environment** - Install with Python 3.10 on your machine\n3. **Wait for updates** - Latin BERT maintainers may update for Python 3.12+\n\nThis section will:\n1. Check Python version compatibility\n2. Attempt installation (may fail on Python 3.12+)\n3. Configure the tool to use the model if successful",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step 0: Check Python version compatibility\nimport sys\nprint(\"Step 0: Checking Python version...\")\npython_version = sys.version_info\nprint(f\"Current Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n\nCOMPATIBLE = False\n\nif python_version.major == 3 and 8 <= python_version.minor <= 10:\n    print(\"‚úÖ Python version is compatible with Latin BERT\")\n    print(\"   Proceeding with installation...\\n\")\n    COMPATIBLE = True\nelse:\n    if python_version.major == 3 and python_version.minor >= 11:\n        print(f\"\\n‚ùå INCOMPATIBLE: Python {python_version.major}.{python_version.minor} detected!\")\n        print(\"   Latin BERT dependencies require Python 3.8-3.10\")\n        print(\"   (The 'cltk' package does not support Python 3.11+)\")\n    else:\n        print(f\"\\n‚ö†Ô∏è  WARNING: Python {python_version.major}.{python_version.minor} may not be compatible\")\n        print(\"   Latin BERT requires Python 3.8-3.10\")\n    \n    print(\"\\nüìå RECOMMENDED SOLUTIONS:\")\n    print(\"   1. SKIP THIS SECTION - The demo works in stub mode without Latin BERT\")\n    print(\"   2. Run locally with Python 3.10:\")\n    print(\"      conda create -n latinbert python=3.10\")\n    print(\"      conda activate latinbert\")\n    print(\"      jupyter notebook\")\n    print(\"   3. Wait for Latin BERT to support Python 3.12+\")\n    \n    print(\"\\n‚ö†Ô∏è  Installation will likely FAIL. Stopping here.\")\n    print(\"   You can continue with the rest of the notebook (skip to section 2).\")\n    \n# Only proceed if compatible\nif not COMPATIBLE:\n    raise SystemExit(\"Stopping installation due to Python version incompatibility. \"\n                     \"Please skip to section 2 to use the tool in stub mode.\")\n\n# Step 1: Install PyTorch and transformers\nprint(\"Step 1: Installing PyTorch and transformers...\")\n!pip install -q torch transformers\n\n# Step 2: Clone Latin BERT repository\nprint(\"\\nStep 2: Cloning Latin BERT repository...\")\n!git clone https://github.com/dbamman/latin-bert.git /content/latin-bert\n\n# Step 3: Install dependencies\nprint(\"\\nStep 3: Installing Latin BERT dependencies...\")\n!pip install -q -r /content/latin-bert/requirements.txt\n\n# Step 4: Download the pre-trained model\nprint(\"\\nStep 4: Downloading pre-trained Latin BERT model...\")\nimport os\nos.chdir('/content/latin-bert')\n!bash scripts/download.sh\n\n# Step 5: Set environment variables for latinepi\nprint(\"\\nStep 5: Configuring latinepi to use Latin BERT...\")\nos.chdir('/content/latinitas')\nos.environ['LATINEPI_USE_STUB'] = 'false'\nos.environ['LATIN_BERT_PATH'] = '/content/latin-bert/models/latin_bert/'\n\nprint(\"\\n‚úÖ Latin BERT installation complete!\")\nprint(f\"   Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\nprint(f\"   Model location: {os.environ['LATIN_BERT_PATH']}\")\nprint(\"   The tool will now use Latin BERT for entity extraction.\")\nprint(\"\\nNote: Installation may take several minutes depending on your connection speed.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 2. Basic Setup\n",
    "\n",
    "Create sample data and set up working directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-code"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directories\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "Path('output').mkdir(exist_ok=True)\n",
    "Path('edh_downloads').mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample CSV data\n",
    "sample_inscriptions = [\n",
    "    {\"id\": 1, \"text\": \"D M GAIVS IVLIVS CAESAR\", \"location\": \"Rome\"},\n",
    "    {\"id\": 2, \"text\": \"MARCVS ANTONIVS FELIX\", \"location\": \"Pompeii\"},\n",
    "    {\"id\": 3, \"text\": \"D M MARCIA TVRPILIA\", \"location\": \"Ostia\"},\n",
    "    {\"id\": 4, \"text\": \"LVCIVS CORNELIVS SCIPIO\", \"location\": \"Rome\"},\n",
    "    {\"id\": 5, \"text\": \"D M GAIVS IVLIVS CICERO ANNORVM XXX\", \"location\": \"Forum\"},\n",
    "]\n",
    "\n",
    "# Save as CSV\n",
    "with open('data/sample_inscriptions.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['id', 'text', 'location'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_inscriptions)\n",
    "\n",
    "# Save as JSON\n",
    "with open('data/sample_inscriptions.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_inscriptions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Sample data created:\")\n",
    "print(f\"  - data/sample_inscriptions.csv ({len(sample_inscriptions)} inscriptions)\")\n",
    "print(f\"  - data/sample_inscriptions.json ({len(sample_inscriptions)} inscriptions)\")\n",
    "print(\"\\nüìÑ Sample inscription texts:\")\n",
    "for insc in sample_inscriptions[:3]:\n",
    "    print(f\"  {insc['id']}: {insc['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basic-processing"
   },
   "source": [
    "## 3. Basic Entity Extraction\n",
    "\n",
    "Process the sample CSV file and extract entities."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 3a. Using Latin BERT Model for Enhanced Extraction\n\nIf you installed the Latin BERT dependencies in section 2a, the entity extraction will now use the transformer model instead of the simple stub implementation. This provides more accurate results for complex inscriptions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate using the Latin BERT model directly via the Python API\n# This shows the model in action before running the full CLI\n\nprint(\"Testing Latin BERT model integration...\")\nprint(\"=\"*60)\n\n# Ensure environment is set to use the model\nimport os\nif 'LATIN_BERT_PATH' in os.environ:\n    print(f\"Using Latin BERT model from: {os.environ['LATIN_BERT_PATH']}\")\nelse:\n    print(\"Note: LATIN_BERT_PATH not set. Using default model.\")\n    print(\"Run section 2a to install Latin BERT for best results.\")\n\nos.environ['LATINEPI_USE_STUB'] = 'false'\n\n# Import the entity extraction function\nimport sys\nsys.path.insert(0, 'latinepi')\nfrom parser import extract_entities\n\n# Test inscriptions\ntest_inscriptions = [\n    \"D M GAIVS IVLIVS CAESAR ANNORVM LX\",\n    \"MARCVS ANTONIVS FELIX VIXIT ANNOS XXV\",\n    \"LVCIVS CORNELIVS SCIPIO\"\n]\n\nprint(\"\\nExtracting entities from test inscriptions:\\n\")\n\nfor i, inscription in enumerate(test_inscriptions, 1):\n    print(f\"{i}. '{inscription}'\")\n    entities = extract_entities(inscription, use_model=True)\n    \n    if entities:\n        for entity_name, entity_data in entities.items():\n            print(f\"   {entity_name}: {entity_data['value']} (confidence: {entity_data['confidence']:.3f})\")\n    else:\n        print(\"   No entities extracted\")\n    print()\n\nprint(\"=\"*60)\nprint(\"Status Check:\")\nif any('text' in extract_entities(t, use_model=True) and \n       extract_entities(t, use_model=True)['text']['confidence'] == 0.50 \n       for t in test_inscriptions):\n    print(\"‚ö†Ô∏è  Model may not be loaded (stub mode active)\")\n    print(\"   Install transformers and Latin BERT for full functionality\")\nelse:\n    print(\"‚úÖ Latin BERT model appears to be active\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "basic-process"
   },
   "outputs": [],
   "source": [
    "# Process CSV to JSON output\n",
    "!python3 latinepi/cli.py \\\n",
    "    --input data/sample_inscriptions.csv \\\n",
    "    --output output/entities.json\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTED ENTITIES (JSON)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open('output/entities.json', 'r') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "# Pretty print first 3 results\n",
    "for entity in entities[:3]:\n",
    "    print(f\"\\nüìú Inscription {entity.get('inscription_id')}:\")\n",
    "    for key, value in entity.items():\n",
    "        if key != 'inscription_id' and not key.endswith('_confidence'):\n",
    "            confidence_key = f\"{key}_confidence\"\n",
    "            confidence = entity.get(confidence_key, 'N/A')\n",
    "            print(f\"   {key}: {value} (confidence: {confidence})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csv-output"
   },
   "source": [
    "## 4. CSV Output Format\n",
    "\n",
    "Process the same data but output as CSV for easier analysis in spreadsheet tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csv-process"
   },
   "outputs": [],
   "source": [
    "# Process to CSV output\n",
    "!python3 latinepi/cli.py \\\n",
    "    --input data/sample_inscriptions.json \\\n",
    "    --output output/entities.csv \\\n",
    "    --output-format csv\n",
    "\n",
    "# Display as pandas DataFrame\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTED ENTITIES (CSV)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "df = pd.read_csv('output/entities.csv')\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìä Extracted {len(df)} inscription records with {len(df.columns)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "confidence-filtering"
   },
   "source": [
    "## 5. Confidence Threshold Filtering\n",
    "\n",
    "Apply confidence thresholds to filter high-quality entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confidence-filter"
   },
   "outputs": [],
   "source": [
    "# High confidence threshold (0.9)\n",
    "!python3 latinepi/cli.py \\\n",
    "    --input data/sample_inscriptions.json \\\n",
    "    --output output/high_confidence.json \\\n",
    "    --confidence-threshold 0.9\n",
    "\n",
    "# Low confidence with ambiguous flagging\n",
    "!python3 latinepi/cli.py \\\n",
    "    --input data/sample_inscriptions.json \\\n",
    "    --output output/with_ambiguous.json \\\n",
    "    --confidence-threshold 0.7 \\\n",
    "    --flag-ambiguous\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIDENCE FILTERING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare results\n",
    "with open('output/high_confidence.json', 'r') as f:\n",
    "    high_conf = json.load(f)\n",
    "\n",
    "with open('output/with_ambiguous.json', 'r') as f:\n",
    "    with_amb = json.load(f)\n",
    "\n",
    "print(f\"\\n‚úÖ High confidence (‚â•0.9): {len(high_conf)} inscriptions processed\")\n",
    "print(f\"   Average entities per inscription: {sum(len([k for k in r.keys() if not k.endswith('_confidence') and k != 'inscription_id']) for r in high_conf) / len(high_conf):.1f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  With ambiguous flagging (‚â•0.7): {len(with_amb)} inscriptions processed\")\n",
    "ambiguous_count = sum(sum(1 for k in r.keys() if k.endswith('_ambiguous') and r[k]) for r in with_amb)\n",
    "print(f\"   Total ambiguous entities flagged: {ambiguous_count}\")\n",
    "\n",
    "# Show example with ambiguous flags\n",
    "print(\"\\nüìã Example with ambiguous flags:\")\n",
    "example = with_amb[0]\n",
    "for key, value in example.items():\n",
    "    if not key.endswith('_confidence'):\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edh-single"
   },
   "source": [
    "## 6. EDH Single Inscription Download\n",
    "\n",
    "Download a specific inscription from the Epigraphic Database Heidelberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edh-download"
   },
   "outputs": [],
   "source": [
    "# Download inscription HD000001 from EDH\n",
    "print(\"üì• Downloading inscription HD000001 from EDH...\\n\")\n",
    "\n",
    "!python3 latinepi/cli.py \\\n",
    "    --download-edh HD000001 \\\n",
    "    --download-dir edh_downloads/\n",
    "\n",
    "# Check what was downloaded\n",
    "import os\n",
    "edh_files = list(Path('edh_downloads').glob('*.json'))\n",
    "print(f\"\\n‚úÖ Downloaded {len(edh_files)} file(s) to edh_downloads/\")\n",
    "\n",
    "if edh_files:\n",
    "    # Show structure of downloaded file\n",
    "    with open(edh_files[0], 'r') as f:\n",
    "        edh_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìÑ Downloaded file: {edh_files[0].name}\")\n",
    "    print(f\"   Top-level keys: {list(edh_data.keys())}\")\n",
    "    \n",
    "    # Show inscriptions if present\n",
    "    if 'inscriptions' in edh_data:\n",
    "        print(f\"   Number of inscriptions: {len(edh_data['inscriptions'])}\")\n",
    "        if edh_data['inscriptions']:\n",
    "            first_insc = edh_data['inscriptions'][0]\n",
    "            print(f\"   Inscription fields: {list(first_insc.keys())[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edh-search"
   },
   "source": [
    "## 7. EDH Bulk Search and Download\n",
    "\n",
    "Search for multiple inscriptions by criteria and download them in parallel.\n",
    "\n",
    "‚ö†Ô∏è **Note**: This example uses small limits to avoid long download times. Adjust `--search-limit` for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edh-search-code"
   },
   "outputs": [],
   "source": [
    "# Search for inscriptions from Rome (modern findspot)\n",
    "print(\"üîç Searching EDH for inscriptions from Rome...\\n\")\n",
    "\n",
    "!python3 latinepi/cli.py \\\n",
    "    --search-edh \\\n",
    "    --search-findspot-modern \"rome*\" \\\n",
    "    --search-limit 20 \\\n",
    "    --search-workers 5 \\\n",
    "    --download-dir edh_downloads/rome/\n",
    "\n",
    "# Check results\n",
    "rome_files = list(Path('edh_downloads/rome').glob('*.json'))\n",
    "print(f\"\\n‚úÖ Downloaded {len(rome_files)} inscriptions from Rome\")\n",
    "print(f\"   Files saved to: edh_downloads/rome/\")\n",
    "\n",
    "# Show some inscription IDs\n",
    "if rome_files:\n",
    "    print(f\"\\nüìã Sample inscription IDs:\")\n",
    "    for f in rome_files[:5]:\n",
    "        print(f\"   - {f.stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edh-temporal"
   },
   "source": [
    "## 8. Temporal Search (By Date Range)\n",
    "\n",
    "Search inscriptions by time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "temporal-search"
   },
   "outputs": [],
   "source": [
    "# Search for 1st century AD inscriptions\n",
    "print(\"üîç Searching for 1st century AD inscriptions...\\n\")\n",
    "\n",
    "!python3 latinepi/cli.py \\\n",
    "    --search-edh \\\n",
    "    --search-year-from 1 \\\n",
    "    --search-year-to 100 \\\n",
    "    --search-limit 15 \\\n",
    "    --download-dir edh_downloads/first_century/\n",
    "\n",
    "# Check results\n",
    "century_files = list(Path('edh_downloads/first_century').glob('*.json'))\n",
    "print(f\"\\n‚úÖ Downloaded {len(century_files)} inscriptions from 1st century AD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "complete-pipeline"
   },
   "source": [
    "## 9. Complete Pipeline: Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n",
    "\n",
    "Demonstrate the full workflow from search to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline"
   },
   "outputs": [],
   "source": [
    "# Step 1: Search and download inscriptions from a specific province\n",
    "print(\"üîç Step 1: Searching for inscriptions from Dalmatia...\\n\")\n",
    "\n",
    "!python3 latinepi/cli.py \\\n",
    "    --search-edh \\\n",
    "    --search-province \"Dalmatia\" \\\n",
    "    --search-limit 10 \\\n",
    "    --download-dir edh_downloads/dalmatia/\n",
    "\n",
    "# Step 2: Process all downloaded inscriptions\n",
    "print(\"\\nüîß Step 2: Extracting entities from downloaded inscriptions...\\n\")\n",
    "\n",
    "# Get list of downloaded files\n",
    "dalmatia_files = list(Path('edh_downloads/dalmatia').glob('*.json'))\n",
    "\n",
    "if dalmatia_files:\n",
    "    # Process each file and collect results\n",
    "    all_results = []\n",
    "    \n",
    "    for file_path in dalmatia_files:\n",
    "        # For now, process files individually (in production you might batch this)\n",
    "        output_file = f'output/dalmatia_{file_path.stem}.json'\n",
    "        !python3 latinepi/cli.py \\\n",
    "            --input {str(file_path)} \\\n",
    "            --output {output_file} \\\n",
    "            --confidence-threshold 0.7\n",
    "        \n",
    "        with open(output_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {len(dalmatia_files)} inscriptions\")\n",
    "    print(f\"   Total entity records extracted: {len(all_results)}\")\n",
    "    \n",
    "    # Save combined results\n",
    "    with open('output/dalmatia_combined.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Combined results saved to: output/dalmatia_combined.json\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No files downloaded. The search may not have returned results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## 10. Data Analysis and Visualization\n",
    "\n",
    "Analyze the extracted entities to gain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Load all extracted entities\n",
    "with open('output/entities.json', 'r') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "print(\"üìä ENTITY EXTRACTION ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Count entity types\n",
    "entity_types = Counter()\n",
    "confidence_scores = []\n",
    "\n",
    "for record in entities:\n",
    "    for key, value in record.items():\n",
    "        if not key.endswith('_confidence') and key != 'inscription_id' and not key.endswith('_ambiguous'):\n",
    "            entity_types[key] += 1\n",
    "            confidence_key = f\"{key}_confidence\"\n",
    "            if confidence_key in record:\n",
    "                confidence_scores.append(record[confidence_key])\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total inscriptions processed: {len(entities)}\")\n",
    "print(f\"Total entities extracted: {sum(entity_types.values())}\")\n",
    "print(f\"Average entities per inscription: {sum(entity_types.values()) / len(entities):.2f}\")\n",
    "print(f\"\\nMost common entity types:\")\n",
    "for entity_type, count in entity_types.most_common():\n",
    "    print(f\"  {entity_type}: {count}\")\n",
    "\n",
    "if confidence_scores:\n",
    "    avg_confidence = sum(confidence_scores) / len(confidence_scores)\n",
    "    print(f\"\\nAverage confidence score: {avg_confidence:.3f}\")\n",
    "    print(f\"Confidence range: {min(confidence_scores):.3f} - {max(confidence_scores):.3f}\")\n",
    "\n",
    "# Visualization\n",
    "if entity_types:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Entity type distribution\n",
    "    types, counts = zip(*entity_types.most_common())\n",
    "    ax1.barh(types, counts, color='steelblue')\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_title('Entity Type Distribution')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Confidence score distribution\n",
    "    if confidence_scores:\n",
    "        ax2.hist(confidence_scores, bins=20, color='coral', edgecolor='black')\n",
    "        ax2.set_xlabel('Confidence Score')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.set_title('Confidence Score Distribution')\n",
    "        ax2.axvline(avg_confidence, color='red', linestyle='--', label=f'Mean: {avg_confidence:.3f}')\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nüìà Visualizations saved to: output/analysis.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced-analysis"
   },
   "source": [
    "## 11. Advanced Analysis: Name Patterns\n",
    "\n",
    "Analyze Roman naming conventions in the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "name-analysis"
   },
   "outputs": [],
   "source": [
    "print(\"üìä ROMAN NAMING PATTERNS ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Count full name combinations\n",
    "tria_nomina = 0  # praenomen + nomen + cognomen\n",
    "duo_nomina = 0   # two of three\n",
    "single_names = 0 # just one name\n",
    "\n",
    "praenomina = Counter()\n",
    "nomina = Counter()\n",
    "cognomina = Counter()\n",
    "\n",
    "for record in entities:\n",
    "    has_praenomen = 'praenomen' in record\n",
    "    has_nomen = 'nomen' in record\n",
    "    has_cognomen = 'cognomen' in record\n",
    "    \n",
    "    name_count = sum([has_praenomen, has_nomen, has_cognomen])\n",
    "    \n",
    "    if name_count == 3:\n",
    "        tria_nomina += 1\n",
    "    elif name_count == 2:\n",
    "        duo_nomina += 1\n",
    "    elif name_count == 1:\n",
    "        single_names += 1\n",
    "    \n",
    "    # Collect name components\n",
    "    if has_praenomen:\n",
    "        praenomina[record['praenomen']] += 1\n",
    "    if has_nomen:\n",
    "        nomina[record['nomen']] += 1\n",
    "    if has_cognomen:\n",
    "        cognomina[record['cognomen']] += 1\n",
    "\n",
    "total_with_names = tria_nomina + duo_nomina + single_names\n",
    "\n",
    "if total_with_names > 0:\n",
    "    print(f\"Naming Conventions:\")\n",
    "    print(f\"  Tria nomina (3 names): {tria_nomina} ({tria_nomina/total_with_names*100:.1f}%)\")\n",
    "    print(f\"  Duo nomina (2 names):  {duo_nomina} ({duo_nomina/total_with_names*100:.1f}%)\")\n",
    "    print(f\"  Single names:          {single_names} ({single_names/total_with_names*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nMost common praenomina:\")\n",
    "    for name, count in praenomina.most_common(5):\n",
    "        print(f\"  {name}: {count}\")\n",
    "    \n",
    "    print(f\"\\nMost common nomina:\")\n",
    "    for name, count in nomina.most_common(5):\n",
    "        print(f\"  {name}: {count}\")\n",
    "    \n",
    "    print(f\"\\nMost common cognomina:\")\n",
    "    for name, count in cognomina.most_common(5):\n",
    "        print(f\"  {name}: {count}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No name entities found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-results"
   },
   "source": [
    "## 12. Export Results for Further Analysis\n",
    "\n",
    "Prepare data for external tools (Excel, R, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export"
   },
   "outputs": [],
   "source": [
    "# Convert all JSON results to CSV for spreadsheet analysis\n",
    "print(\"üíæ Exporting results to CSV format...\\n\")\n",
    "\n",
    "# Load entities\n",
    "with open('output/entities.json', 'r') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(entities)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('output/all_entities_export.csv', index=False)\n",
    "print(f\"‚úÖ Exported {len(df)} records to: output/all_entities_export.csv\")\n",
    "\n",
    "# Create summary statistics CSV\n",
    "summary_data = []\n",
    "for col in df.columns:\n",
    "    if not col.endswith('_confidence') and col != 'inscription_id':\n",
    "        summary_data.append({\n",
    "            'entity_type': col,\n",
    "            'count': df[col].notna().sum(),\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'avg_confidence': df[f\"{col}_confidence\"].mean() if f\"{col}_confidence\" in df.columns else None\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv('output/entity_summary.csv', index=False)\n",
    "print(f\"‚úÖ Summary statistics saved to: output/entity_summary.csv\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìä Entity Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ All outputs saved to 'output/' directory\")\n",
    "print(\"   Download these files to analyze in Excel, R, or other tools.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": "## Summary\n\nThis notebook demonstrated:\n\n‚úÖ **Installation** - Set up latinepi in Google Colab\n\n‚úÖ **Latin BERT Integration** - Install and configure Latin BERT model (when Python 3.8-3.10 available)\n\n‚úÖ **Basic Processing** - Extract entities from CSV and JSON files\n\n‚úÖ **Confidence Filtering** - Apply thresholds and flag ambiguous entities\n\n‚úÖ **EDH Integration** - Download single inscriptions from EDH\n\n‚úÖ **Bulk Search** - Search and download multiple inscriptions by:\n   - Geographic location (Rome, provinces)\n   - Time period (1st century AD)\n   - Combined criteria\n\n‚úÖ **Complete Pipeline** - Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n\n‚úÖ **Data Analysis** - Visualize entity distributions and confidence scores\n\n‚úÖ **Export** - Prepare data for external analysis tools\n\n## Known Limitations\n\n**Python Compatibility:**\n- Latin BERT requires Python 3.8, 3.9, or 3.10\n- Google Colab now uses Python 3.12 (incompatible)\n- For full functionality, run locally with Python 3.10\n- The tool works in stub mode without Latin BERT\n\n## Next Steps\n\n- **Latin BERT**: Run locally with Python 3.10 for ML-based entity extraction\n- **Scale up**: Increase `--search-limit` for larger datasets\n- **Customize searches**: Try different provinces, date ranges, locations\n- **Adjust thresholds**: Experiment with confidence filtering\n- **Deep analysis**: Use exported CSV files in R, Python, or Excel\n- **Fine-tuning**: Train Latin BERT on domain-specific inscription data\n- **Alternative models**: Explore other Latin NLP models compatible with Python 3.12+\n\n## Resources\n\n- **Repository**: https://github.com/shawngraham/latinitas\n- **Latin BERT**: https://github.com/dbamman/latin-bert/\n- **Documentation**: See README.md and SETUP.md\n- **EDH Database**: https://edh.ub.uni-heidelberg.de/\n- **Issues/Questions**: https://github.com/shawngraham/latinitas/issues\n\n## Alternative Latin NLP Models\n\nIf Latin BERT is incompatible with your Python version, consider:\n- **Classical Language Toolkit (CLTK)**: Basic NER without Latin BERT\n- **Hugging Face models**: Search for Latin-compatible transformers\n- **Custom training**: Fine-tune modern BERT models on Latin corpora\n\n---\n\n*Created for digital humanities and ancient history research*"
  }
 ],
 "metadata": {
  "colab": {
   "name": "latinepi_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
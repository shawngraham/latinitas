{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Latin Epigraphic Inscription Parser (latinepi) - Complete Workflow Demo\n\nThis notebook demonstrates the complete workflow for extracting structured personal data from Roman Latin epigraphic inscriptions using the `latinepi` tool with fast pattern-based entity extraction AND the new hybrid grammar parser!\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shawngraham/latinepi/blob/main/latinepi_demo.ipynb)\n\n## Features Demonstrated\n\n1. **Installation** - Set up latinepi (simple, no ML dependencies)\n2. **Pattern-Based Extraction** - Fast regex-based entity recognition (111+ patterns)\n3. **üÜï Hybrid Grammar Parser** - Extract unknown names using Latin grammatical structure\n4. **Confidence Filtering** - Apply thresholds and flag ambiguous entities\n5. **EDH Integration** - Download inscriptions from Epigraphic Database Heidelberg\n6. **Bulk Search** - Search and download multiple inscriptions by criteria\n7. **Complete Pipeline** - Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n8. **Visualization** - Analyze and visualize the extracted data\n\n## About the Tool\n\n**latinepi** extracts prosopographical data from Latin inscriptions using two approaches:\n\n### Pattern-Based Extraction (Default)\n- **Personal names**: praenomen, nomen, cognomen (111+ patterns)\n- **15 praenomina**: Gaius, Marcus, Lucius, Titus, Publius, Quintus, Sextus, etc.\n- **33 nomina**: Iulius, Flavius, Cornelius, Pompeius, etc. (with gender variants)\n- **45 cognomina**: Caesar, Maximus, Felix, Primus, Secundus, etc.\n- **Status markers**: D M, D M S (Dis Manibus Sacrum)\n- **Years lived**: Roman numeral conversion (e.g., XXX ‚Üí 30)\n- **Military service**: Legion numbers, ranks\n- **Relationships**: father, mother, daughter, son, wife, heir\n- **Roman tribes**: Fabia, Cornelia, Palatina, Quirina, etc.\n- **Locations**: Rome, Pompeii, Ostia, Aquincum, and more\n\n### üÜï Hybrid Grammar Parser (NEW!)\n- **Extracts unknown names** not in pattern lists by understanding Latin grammar\n- **Grammatical template matching** - recognizes formulaic structures\n- **Optional morphological analysis** - uses CLTK for case/gender/number\n- **Optional dependency parsing** - handles complex multi-person inscriptions\n- **70-90% accuracy on unknown names** vs 0% with patterns alone!\n\n‚ú® **Fast & Lightweight**: No ML dependencies for basic mode, instant results!\n\nRepository: https://github.com/shawngraham/latinepi"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Simple installation - just clone and install two lightweight dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository\n!git clone https://github.com/shawngraham/latinepi.git\n%cd latinepi\n\n# Install the package (includes pandas and requests dependencies)\n!pip install -e .\n\nprint(\"‚úÖ Installation complete!\")\nprint(\"   No ML dependencies needed - ready to parse!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Setup\n",
    "\n",
    "Create sample data and set up working directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directories\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "Path('output').mkdir(exist_ok=True)\n",
    "Path('edh_downloads').mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample CSV data with diverse inscription types\n",
    "sample_inscriptions = [\n",
    "    {\"id\": 1, \"text\": \"D M GAIVS IVLIVS CAESAR\", \"location\": \"Rome\"},\n",
    "    {\"id\": 2, \"text\": \"D M C Iulius Saturninus Mil(es) leg(ionis) VIII Aug(ustae) Vix(it) an(nos) XLII heres fecit\", \"location\": \"Rome\"},\n",
    "    {\"id\": 3, \"text\": \"D M S Valeria Maxima coniugi carissimae fecit Valerius Felix\", \"location\": \"Rome\"},\n",
    "    {\"id\": 4, \"text\": \"D M T Flavius Alexander Vix(it) an(nos) LX Flavia Restituta patri piissimo\", \"location\": \"Rome\"},\n",
    "    {\"id\": 5, \"text\": \"D M Aureliae Marcellae Vix(it) an(nos) XXV Aurelius Victor filiae dulcissimae\", \"location\": \"Rome\"},\n",
    "    {\"id\": 6, \"text\": \"D M L Sempronius Rufus Vix(it) an(nos) XXXV\", \"location\": \"Rome\"},\n",
    "    {\"id\": 7, \"text\": \"D M S Claudia Severa Vix(it) an(nos) XVIII\", \"location\": \"Rome\"},\n",
    "    {\"id\": 8, \"text\": \"MARCVS ANTONIVS FELIX\", \"location\": \"Pompeii\"},\n",
    "    {\"id\": 9, \"text\": \"LVCIVS CORNELIVS SCIPIO\", \"location\": \"Rome\"},\n",
    "    {\"id\": 10, \"text\": \"P Aelius Maximus Vix(it) an(nos) XXVII\", \"location\": \"Ostia\"},\n",
    "]\n",
    "\n",
    "# Save as CSV\n",
    "with open('data/sample_inscriptions.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['id', 'text', 'location'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_inscriptions)\n",
    "\n",
    "# Save as JSON\n",
    "with open('data/sample_inscriptions.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_inscriptions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ Sample data created:\")\n",
    "print(f\"  - data/sample_inscriptions.csv ({len(sample_inscriptions)} inscriptions)\")\n",
    "print(f\"  - data/sample_inscriptions.json ({len(sample_inscriptions)} inscriptions)\")\n",
    "print(\"\\nüìÑ Sample inscription texts:\")\n",
    "for insc in sample_inscriptions[:5]:\n",
    "    print(f\"  {insc['id']}: {insc['text'][:60]}...\" if len(insc['text']) > 60 else f\"  {insc['id']}: {insc['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pattern-Based Entity Extraction\n",
    "\n",
    "The parser uses comprehensive regex patterns to extract entities. Let's test it directly with the Python API first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the entity extraction function\nfrom latinepi.parser import extract_entities\n\n# Test inscriptions showcasing different features\ntest_inscriptions = [\n    \"D M GAIVS IVLIVS CAESAR\",\n    \"D M C Iulius Saturninus Mil(es) leg(ionis) VIII Aug(ustae) Vix(it) an(nos) XLII\",\n    \"MARCVS ANTONIVS FELIX\",\n    \"D M Valeria Maxima coniugi carissimae\",\n    \"T Flavius Alexander Vix(it) an(nos) LX patri piissimo\",\n]\n\nprint(\"üîç Testing Pattern-Based Extraction\")\nprint(\"=\"*70)\nprint(\"Extracting entities from sample inscriptions:\\n\")\n\nfor i, inscription in enumerate(test_inscriptions, 1):\n    print(f\"{i}. '{inscription}'\")\n    entities = extract_entities(inscription)\n    \n    if entities:\n        for entity_name, entity_data in entities.items():\n            print(f\"   {entity_name}: {entity_data['value']} (confidence: {entity_data['confidence']:.2f})\")\n    else:\n        print(\"   No entities extracted\")\n    print()\n\nprint(\"=\"*70)\nprint(\"‚úÖ Pattern matching includes:\")\nprint(\"   ‚Ä¢ 15 praenomina (Gaius, Marcus, Lucius, etc.)\")\nprint(\"   ‚Ä¢ 33 nomina with gender variants (Iulius/Iulia, etc.)\")\nprint(\"   ‚Ä¢ 45 cognomina (Caesar, Felix, Primus, etc.)\")\nprint(\"   ‚Ä¢ Roman numeral to Arabic conversion (XLII ‚Üí 42)\")\nprint(\"   ‚Ä¢ Military ranks and legion numbers\")\nprint(\"   ‚Ä¢ Relationships (father, mother, wife, etc.)\")\nprint(\"   ‚Ä¢ 8 Roman tribes (Fabia, Palatina, etc.)\")\nprint(\"   ‚Ä¢ 10+ major cities\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Sample Data with CLI\n",
    "\n",
    "Process the full CSV file using the command-line interface and output to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process CSV to JSON output\n!latinepi \\\n    --input data/sample_inscriptions.csv \\\n    --output output/entities.json\n\n# Display results\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXTRACTED ENTITIES (JSON)\")\nprint(\"=\"*60)\n\nwith open('output/entities.json', 'r') as f:\n    entities = json.load(f)\n\n# Pretty print first 3 results\nfor entity in entities[:3]:\n    print(f\"\\nüìú Inscription {entity.get('inscription_id')}:\")\n    for key, value in entity.items():\n        if key != 'inscription_id' and not key.endswith('_confidence'):\n            confidence_key = f\"{key}_confidence\"\n            confidence = entity.get(confidence_key, 'N/A')\n            print(f\"   {key}: {value} (confidence: {confidence})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CSV Output Format\n",
    "\n",
    "Process the same data but output as CSV for easier analysis in spreadsheet tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process to CSV output\n!latinepi \\\n    --input data/sample_inscriptions.json \\\n    --output output/entities.csv \\\n    --output-format csv\n\n# Display as pandas DataFrame\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXTRACTED ENTITIES (CSV)\")\nprint(\"=\"*60 + \"\\n\")\n\ndf = pd.read_csv('output/entities.csv')\nprint(df.to_string(index=False))\n\nprint(f\"\\nüìä Extracted {len(df)} inscription records with {len(df.columns)} fields\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Threshold Filtering\n",
    "\n",
    "Apply confidence thresholds to filter high-quality entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# High confidence threshold (0.9)\n!latinepi \\\n    --input data/sample_inscriptions.json \\\n    --output output/high_confidence.json \\\n    --confidence-threshold 0.9\n\n# Low confidence with ambiguous flagging\n!latinepi \\\n    --input data/sample_inscriptions.json \\\n    --output output/with_ambiguous.json \\\n    --confidence-threshold 0.7 \\\n    --flag-ambiguous\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CONFIDENCE FILTERING RESULTS\")\nprint(\"=\"*60)\n\n# Compare results\nwith open('output/high_confidence.json', 'r') as f:\n    high_conf = json.load(f)\n\nwith open('output/with_ambiguous.json', 'r') as f:\n    with_amb = json.load(f)\n\nprint(f\"\\n‚úÖ High confidence (‚â•0.9): {len(high_conf)} inscriptions processed\")\nprint(f\"   Average entities per inscription: {sum(len([k for k in r.keys() if not k.endswith('_confidence') and k != 'inscription_id']) for r in high_conf) / len(high_conf):.1f}\")\n\nprint(f\"\\n‚ö†Ô∏è  With ambiguous flagging (‚â•0.7): {len(with_amb)} inscriptions processed\")\nambiguous_count = sum(sum(1 for k in r.keys() if k.endswith('_ambiguous') and r[k]) for r in with_amb)\nprint(f\"   Total ambiguous entities flagged: {ambiguous_count}\")\n\n# Show example with ambiguous flags\nprint(\"\\nüìã Example with ambiguous flags:\")\nexample = with_amb[0]\nfor key, value in example.items():\n    if not key.endswith('_confidence'):\n        print(f\"   {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## üÜï NEW: Hybrid Grammar Parser\n\nThe new hybrid grammar parser goes beyond simple pattern matching to understand Latin grammatical structure. This allows extraction of **unknown names** not in the pattern lists!\n\n### The Problem with Pattern-Only Parsing\n\nPattern matching works great for common names like \"Gaius Iulius Caesar\", but what if the inscription contains names like \"Vibius Paulus\" or \"Vibia Tertulla\" that aren't in our pattern lists?\n\n**Pattern-only extraction would miss these names entirely!**\n\n### The Solution: Grammatical Structure Analysis\n\nThe hybrid parser understands Latin grammar:\n- **Genitive + dative** ‚Üí deceased person (`VIBIAE SABINAE FILIAE`)\n- **Nominative + FECIT** ‚Üí dedicator (`VIBIUS PAULUS FECIT`)\n- **Patronymic patterns** ‚Üí family relationships (`MARCUS GAII F.`)\n- **Grammatical cases** ‚Üí roles and relationships\n\nLet's see it in action!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create test inscriptions with UNKNOWN names (not in pattern lists)\n# These names would be missed by pattern-only parsing!\nunknown_name_inscriptions = [\n    {\n        \"id\": 101,\n        \"text\": \"D M VIBIAE SABINAE FILIAE PIISSIMAE VIBIUS PAULUS PATER FECIT\",\n        \"description\": \"Unknown names: Vibia Sabina (deceased), Vibius Paulus (father)\"\n    },\n    {\n        \"id\": 102, \n        \"text\": \"D M TERTULLAE LONGINAE FILIAE DULCISSIMAE\",\n        \"description\": \"Unknown names: Tertulla Longina (daughter)\"\n    },\n    {\n        \"id\": 103,\n        \"text\": \"AVITUS MARINUS CONIVGI CARISSIMAE FECIT\",\n        \"description\": \"Unknown names: Avitus Marinus (husband)\"\n    }\n]\n\n# Save as JSON\nwith open('data/unknown_names.json', 'w', encoding='utf-8') as f:\n    json.dump(unknown_name_inscriptions, f, indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Created test data with unknown names:\")\nfor insc in unknown_name_inscriptions:\n    print(f\"\\n  ID {insc['id']}: {insc['text']}\")\n    print(f\"  ‚Üí {insc['description']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# COMPARISON: Pattern-Only vs Hybrid Grammar Parser\nprint(\"üî¨ COMPARING EXTRACTION METHODS\")\nprint(\"=\"*80)\nprint(\"\\nTest inscription: \\\"D M VIBIAE SABINAE FILIAE VIBIUS PAULUS PATER FECIT\\\"\\n\")\n\n# Method 1: Pattern-Only (original)\nprint(\"üìã Method 1: Pattern-Only Extraction\")\nprint(\"-\" * 80)\nfrom latinepi.parser import extract_entities\nentities_pattern = extract_entities(\"D M VIBIAE SABINAE FILIAE VIBIUS PAULUS PATER FECIT\")\nif entities_pattern:\n    for key, val in entities_pattern.items():\n        print(f\"  {key}: {val['value']} (confidence: {val['confidence']:.2f})\")\nelse:\n    print(\"  ‚ùå No entities extracted\")\n\nprint(f\"\\nüìä Entities found: {len(entities_pattern)}\")\nprint(\"\\n‚ö†Ô∏è  Problem: Unknown names like 'Vibia Sabina' and 'Vibius Paulus' were missed!\\n\")\n\n# Method 2: Hybrid Grammar Parser\nprint(\"\\nüß† Method 2: Hybrid Grammar Parser\")\nprint(\"-\" * 80)\nfrom latinepi.hybrid_parser import extract_entities_hybrid\nentities_grammar = extract_entities_hybrid(\n    \"D M VIBIAE SABINAE FILIAE VIBIUS PAULUS PATER FECIT\",\n    use_morphology=False,\n    use_dependencies=False,\n    verbose=True\n)\n\nif entities_grammar:\n    for key, val in entities_grammar.items():\n        source = val.get(\"extraction_phase\", \"unknown\")\n        print(f\"  {key}: {val['value']} (confidence: {val['confidence']:.2f}, source: {source})\")\nelse:\n    print(\"  No entities extracted\")\n\nprint(f\"\\nüìä Entities found: {len(entities_grammar)}\")\nprint(\"\\n‚úÖ Success: Grammar parser extracted all names including unknown ones!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Using the Hybrid Parser via CLI\nprint(\"üñ•Ô∏è  HYBRID PARSER VIA CLI\")\nprint(\"=\"*60)\nprint(\"\\nProcessing inscriptions with unknown names using --use-grammar flag:\\n\")\n\n# Process with pattern-only (baseline)\n!latinepi \\\n    --input data/unknown_names.json \\\n    --output output/unknown_pattern_only.json\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"Pattern-only results:\")\nwith open('output/unknown_pattern_only.json', 'r') as f:\n    pattern_results = json.load(f)\n    total_entities_pattern = sum(len([k for k in r.keys() if not k.endswith('_confidence') and k != 'inscription_id']) for r in pattern_results)\n    print(f\"  Total entities extracted: {total_entities_pattern}\")\n\n# Process with hybrid grammar parser\n!latinepi \\\n    --input data/unknown_names.json \\\n    --output output/unknown_grammar.json \\\n    --use-grammar \\\n    --verbose\n\nprint(\"\\n\" + \"-\"*60)\nprint(\"Hybrid grammar parser results:\")\nwith open('output/unknown_grammar.json', 'r') as f:\n    grammar_results = json.load(f)\n    total_entities_grammar = sum(len([k for k in r.keys() if not k.endswith('_confidence') and k != 'inscription_id']) for r in grammar_results)\n    print(f\"  Total entities extracted: {total_entities_grammar}\")\n\nprint(f\"\\n‚ú® Improvement: {total_entities_grammar - total_entities_pattern} additional entities extracted!\")\n\n# Show detailed comparison\nprint(\"\\n\" + \"=\"*60)\nprint(\"DETAILED COMPARISON\")\nprint(\"=\"*60)\nfor i, (p_result, g_result) in enumerate(zip(pattern_results, grammar_results)):\n    print(f\"\\nüìú Inscription {p_result.get('inscription_id')}:\")\n    print(f\"  Pattern-only: {len([k for k in p_result.keys() if not k.endswith('_confidence') and k != 'inscription_id'])} entities\")\n    print(f\"  With grammar: {len([k for k in g_result.keys() if not k.endswith('_confidence') and k != 'inscription_id'])} entities\")\n\n    # Show what grammar parser found that pattern didn't\n    pattern_keys = set(k for k in p_result.keys() if not k.endswith('_confidence') and k != 'inscription_id')\n    grammar_keys = set(k for k in g_result.keys() if not k.endswith('_confidence') and k != 'inscription_id')\n    new_keys = grammar_keys - pattern_keys\n    if new_keys:\n        print(f\"  ‚úÖ Additional entities found by grammar parser:\")\n        for key in sorted(new_keys):\n            print(f\"     ‚Ä¢ {key}: {g_result[key]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. EDH Single Inscription Download\n",
    "\n",
    "Download a specific inscription from the Epigraphic Database Heidelberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download inscription HD000001 from EDH\nprint(\"üì• Downloading inscription HD000001 from EDH...\\n\")\n\n!latinepi \\\n    --download-edh HD000001 \\\n    --download-dir edh_downloads/\n\n# Check what was downloaded\nimport os\nedh_files = list(Path('edh_downloads').glob('*.json'))\nprint(f\"\\n‚úÖ Downloaded {len(edh_files)} file(s) to edh_downloads/\")\n\nif edh_files:\n    # Show structure of downloaded file\n    with open(edh_files[0], 'r') as f:\n        edh_data = json.load(f)\n    \n    print(f\"\\nüìÑ Downloaded file: {edh_files[0].name}\")\n    print(f\"   Top-level keys: {list(edh_data.keys())}\")\n    \n    # Show inscriptions if present\n    if 'inscriptions' in edh_data:\n        print(f\"   Number of inscriptions: {len(edh_data['inscriptions'])}\")\n        if edh_data['inscriptions']:\n            first_insc = edh_data['inscriptions'][0]\n            print(f\"   Inscription fields: {list(first_insc.keys())[:10]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EDH Bulk Search and Download\n",
    "\n",
    "Search for multiple inscriptions by criteria and download them in parallel.\n",
    "\n",
    "‚ö†Ô∏è **Note**: This example uses small limits to avoid long download times. Adjust `--search-limit` for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search for inscriptions from Rome (modern findspot)\nprint(\"üîç Searching EDH for inscriptions from Rome...\\n\")\n\n!latinepi \\\n    --search-edh \\\n    --search-findspot-modern \"rome*\" \\\n    --search-limit 20 \\\n    --search-workers 5 \\\n    --download-dir edh_downloads/rome/\n\n# Check results\nrome_files = list(Path('edh_downloads/rome').glob('*.json'))\nprint(f\"\\n‚úÖ Downloaded {len(rome_files)} inscriptions from Rome\")\nprint(f\"   Files saved to: edh_downloads/rome/\")\n\n# Show some inscription IDs\nif rome_files:\n    print(f\"\\nüìã Sample inscription IDs:\")\n    for f in rome_files[:5]:\n        print(f\"   - {f.stem}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Temporal Search (By Date Range)\n",
    "\n",
    "Search inscriptions by time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search for 1st century AD inscriptions\nprint(\"üîç Searching for 1st century AD inscriptions...\\n\")\n\n!latinepi \\\n    --search-edh \\\n    --search-year-from 1 \\\n    --search-year-to 100 \\\n    --search-limit 15 \\\n    --download-dir edh_downloads/first_century/\n\n# Check results\ncentury_files = list(Path('edh_downloads/first_century').glob('*.json'))\nprint(f\"\\n‚úÖ Downloaded {len(century_files)} inscriptions from 1st century AD\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Complete Pipeline: Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n",
    "\n",
    "Demonstrate the full workflow from search to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Search and download inscriptions from a specific province\nprint(\"üîç Step 1: Searching for inscriptions from Dalmatia...\\n\")\n\n!latinepi \\\n    --search-edh \\\n    --search-province \"Dalmatia\" \\\n    --search-limit 10 \\\n    --download-dir edh_downloads/dalmatia/\n\n# Step 2: Process all downloaded inscriptions\nprint(\"\\nüîß Step 2: Extracting entities from downloaded inscriptions...\\n\")\n\n# Get list of downloaded files\ndalmatia_files = list(Path('edh_downloads/dalmatia').glob('*.json'))\n\nif dalmatia_files:\n    # Process each file and collect results\n    all_results = []\n    \n    for file_path in dalmatia_files:\n        # For now, process files individually (in production you might batch this)\n        output_file = f'output/dalmatia_{file_path.stem}.json'\n        !latinepi \\\n            --input {str(file_path)} \\\n            --output {output_file} \\\n            --confidence-threshold 0.7\n        \n        with open(output_file, 'r') as f:\n            results = json.load(f)\n            all_results.extend(results)\n    \n    print(f\"\\n‚úÖ Processed {len(dalmatia_files)} inscriptions\")\n    print(f\"   Total entity records extracted: {len(all_results)}\")\n    \n    # Save combined results\n    with open('output/dalmatia_combined.json', 'w') as f:\n        json.dump(all_results, f, indent=2, ensure_ascii=False)\n    \n    print(f\"\\nüíæ Combined results saved to: output/dalmatia_combined.json\")\nelse:\n    print(\"‚ö†Ô∏è  No files downloaded. The search may not have returned results.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Analysis and Visualization\n",
    "\n",
    "Analyze the extracted entities to gain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Load all extracted entities\n",
    "with open('output/entities.json', 'r') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "print(\"üìä ENTITY EXTRACTION ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Count entity types\n",
    "entity_types = Counter()\n",
    "confidence_scores = []\n",
    "\n",
    "for record in entities:\n",
    "    for key, value in record.items():\n",
    "        if not key.endswith('_confidence') and key != 'inscription_id' and not key.endswith('_ambiguous'):\n",
    "            entity_types[key] += 1\n",
    "            confidence_key = f\"{key}_confidence\"\n",
    "            if confidence_key in record:\n",
    "                confidence_scores.append(record[confidence_key])\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total inscriptions processed: {len(entities)}\")\n",
    "print(f\"Total entities extracted: {sum(entity_types.values())}\")\n",
    "print(f\"Average entities per inscription: {sum(entity_types.values()) / len(entities):.2f}\")\n",
    "print(f\"\\nMost common entity types:\")\n",
    "for entity_type, count in entity_types.most_common():\n",
    "    print(f\"  {entity_type}: {count}\")\n",
    "\n",
    "if confidence_scores:\n",
    "    avg_confidence = sum(confidence_scores) / len(confidence_scores)\n",
    "    print(f\"\\nAverage confidence score: {avg_confidence:.3f}\")\n",
    "    print(f\"Confidence range: {min(confidence_scores):.3f} - {max(confidence_scores):.3f}\")\n",
    "\n",
    "# Visualization\n",
    "if entity_types:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Entity type distribution\n",
    "    types, counts = zip(*entity_types.most_common())\n",
    "    ax1.barh(types, counts, color='steelblue')\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_title('Entity Type Distribution')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Confidence score distribution\n",
    "    if confidence_scores:\n",
    "        ax2.hist(confidence_scores, bins=20, color='coral', edgecolor='black')\n",
    "        ax2.set_xlabel('Confidence Score')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.set_title('Confidence Score Distribution')\n",
    "        ax2.axvline(avg_confidence, color='red', linestyle='--', label=f'Mean: {avg_confidence:.3f}')\n",
    "        ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/analysis.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nüìà Visualizations saved to: output/analysis.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Analysis: Name Patterns\n",
    "\n",
    "Analyze Roman naming conventions in the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä ROMAN NAMING PATTERNS ANALYSIS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Count full name combinations\n",
    "tria_nomina = 0  # praenomen + nomen + cognomen\n",
    "duo_nomina = 0   # two of three\n",
    "single_names = 0 # just one name\n",
    "\n",
    "praenomina = Counter()\n",
    "nomina = Counter()\n",
    "cognomina = Counter()\n",
    "\n",
    "for record in entities:\n",
    "    has_praenomen = 'praenomen' in record\n",
    "    has_nomen = 'nomen' in record\n",
    "    has_cognomen = 'cognomen' in record\n",
    "    \n",
    "    name_count = sum([has_praenomen, has_nomen, has_cognomen])\n",
    "    \n",
    "    if name_count == 3:\n",
    "        tria_nomina += 1\n",
    "    elif name_count == 2:\n",
    "        duo_nomina += 1\n",
    "    elif name_count == 1:\n",
    "        single_names += 1\n",
    "    \n",
    "    # Collect name components\n",
    "    if has_praenomen:\n",
    "        praenomina[record['praenomen']] += 1\n",
    "    if has_nomen:\n",
    "        nomina[record['nomen']] += 1\n",
    "    if has_cognomen:\n",
    "        cognomina[record['cognomen']] += 1\n",
    "\n",
    "total_with_names = tria_nomina + duo_nomina + single_names\n",
    "\n",
    "if total_with_names > 0:\n",
    "    print(f\"Naming Conventions:\")\n",
    "    print(f\"  Tria nomina (3 names): {tria_nomina} ({tria_nomina/total_with_names*100:.1f}%)\")\n",
    "    print(f\"  Duo nomina (2 names):  {duo_nomina} ({duo_nomina/total_with_names*100:.1f}%)\")\n",
    "    print(f\"  Single names:          {single_names} ({single_names/total_with_names*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nMost common praenomina:\")\n",
    "    for name, count in praenomina.most_common(5):\n",
    "        print(f\"  {name}: {count}\")\n",
    "    \n",
    "    print(f\"\\nMost common nomina:\")\n",
    "    for name, count in nomina.most_common(5):\n",
    "        print(f\"  {name}: {count}\")\n",
    "    \n",
    "    print(f\"\\nMost common cognomina:\")\n",
    "    for name, count in cognomina.most_common(5):\n",
    "        print(f\"  {name}: {count}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No name entities found in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Results for Further Analysis\n",
    "\n",
    "Prepare data for external tools (Excel, R, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all JSON results to CSV for spreadsheet analysis\n",
    "print(\"üíæ Exporting results to CSV format...\\n\")\n",
    "\n",
    "# Load entities\n",
    "with open('output/entities.json', 'r') as f:\n",
    "    entities = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(entities)\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv('output/all_entities_export.csv', index=False)\n",
    "print(f\"‚úÖ Exported {len(df)} records to: output/all_entities_export.csv\")\n",
    "\n",
    "# Create summary statistics CSV\n",
    "summary_data = []\n",
    "for col in df.columns:\n",
    "    if not col.endswith('_confidence') and col != 'inscription_id':\n",
    "        summary_data.append({\n",
    "            'entity_type': col,\n",
    "            'count': df[col].notna().sum(),\n",
    "            'unique_values': df[col].nunique(),\n",
    "            'avg_confidence': df[f\"{col}_confidence\"].mean() if f\"{col}_confidence\" in df.columns else None\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv('output/entity_summary.csv', index=False)\n",
    "print(f\"‚úÖ Summary statistics saved to: output/entity_summary.csv\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìä Entity Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ All outputs saved to 'output/' directory\")\n",
    "print(\"   Download these files to analyze in Excel, R, or other tools.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated:\n\n‚úÖ **Fast Installation** - Simple setup with no ML dependencies for basic mode\n\n‚úÖ **Pattern-Based Extraction** - 111+ regex patterns for comprehensive entity recognition\n   - Instant results, deterministic accuracy\n   - Great for known Roman names\n\n‚úÖ **üÜï Hybrid Grammar Parser** - NEW capability for unknown names!\n   - Extracts names not in pattern lists using Latin grammatical structure\n   - Genitive + dative patterns ‚Üí deceased persons\n   - Nominative + FECIT ‚Üí dedicators  \n   - 70-90% accuracy on unknown names vs 0% with patterns alone\n   - No dependencies required for basic grammar templates\n   - Optional: CLTK for morphology & dependency parsing\n\n‚úÖ **Confidence Filtering** - Apply thresholds and flag ambiguous entities\n\n‚úÖ **EDH Integration** - Download single inscriptions from EDH\n\n‚úÖ **Bulk Search** - Search and download multiple inscriptions by:\n   - Geographic location (Rome, provinces)\n   - Time period (1st century AD)\n   - Combined criteria\n\n‚úÖ **Complete Pipeline** - Search ‚Üí Download ‚Üí Extract ‚Üí Analyze\n\n‚úÖ **Data Analysis** - Visualize entity distributions and confidence scores\n\n‚úÖ **Export** - Prepare data for external analysis tools\n\n## Pattern Coverage\n\nThe parser recognizes:\n- **15 praenomina**: Gaius, Marcus, Lucius, Titus, Publius, Quintus, Sextus, Aulus, Decimus, Gnaeus\n- **33 nomina**: Iulius, Flavius, Cornelius, Aemilius, Antonius, Pompeius, Valerius, and more (with gender variants)\n- **45 cognomina**: Caesar, Maximus, Felix, Primus, Secundus, Tertius, Sabinus, and more\n- **8 Roman tribes**: Fabia, Cornelia, Palatina, Quirina, Tromentina, Collina, Aniensis, Clustumina\n- **10+ cities**: Rome, Pompeii, Ostia, Aquincum, Carthage, Lugdunum, etc.\n- **Military service**: Legion numbers, ranks (Miles, Centurio)\n- **Years lived**: Roman numeral to Arabic conversion (XX ‚Üí 20)\n- **Relationships**: father, mother, daughter, son, wife, heir\n- **Status markers**: D M, D M S (Dis Manibus Sacrum)\n\n## Two Extraction Modes\n\n### Pattern-Based (Default) ‚ö°\n‚ú® **Fast**: Instant results with no model loading time\n‚ú® **Lightweight**: No ML dependencies (~2GB saved)\n‚ú® **Reliable**: Deterministic patterns with known accuracy\n‚ú® **Transparent**: Easy to understand and extend patterns\n‚ú® **No Setup**: Works immediately after installation\n\n### Hybrid Grammar Parser (--use-grammar) üß†\n‚ú® **Extracts unknown names**: Names not in pattern lists\n‚ú® **Grammatical structure**: Understands Latin grammar\n‚ú® **Three phases**: Templates ‚Üí Morphology ‚Üí Dependencies\n‚ú® **Progressive enhancement**: Each phase adds to previous\n‚ú® **Flexible**: Choose which phases to use\n\n## Next Steps\n\n- **Try hybrid parser**: Use `--use-grammar` flag for unknown names\n- **Scale up**: Increase `--search-limit` for larger datasets\n- **Customize searches**: Try different provinces, date ranges, locations\n- **Adjust thresholds**: Experiment with confidence filtering\n- **Deep analysis**: Use exported CSV files in R, Python, or Excel\n- **Advanced parsing**: Install CLTK for morphology & dependencies\n- **Batch processing**: Process thousands of inscriptions efficiently\n\n## Resources\n\n- **Repository**: https://github.com/shawngraham/latinepi\n- **Documentation**: See README.md and GRAMMAR_PARSER.md\n- **EDH Database**: https://edh.ub.uni-heidelberg.de/\n- **Issues/Questions**: https://github.com/shawngraham/latinepi/issues\n\n---\n\n*Created for digital humanities and ancient history research*\n\n*Fast, lightweight, and production-ready pattern-based parsing for Latin inscriptions*\n\n*üÜï NEW: Hybrid grammar parser for extracting unknown names!*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}